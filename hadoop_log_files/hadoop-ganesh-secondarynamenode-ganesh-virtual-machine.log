2023-02-07 21:26:39,474 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-07 21:26:39,485 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-07 21:26:40,247 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-07 21:26:40,374 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-07 21:26:40,374 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-07 21:26:40,651 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-07 21:26:40,664 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 8860@ganesh-virtual-machine
2023-02-07 21:26:40,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-07 21:26:40,675 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-07 21:26:40,678 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-07 21:26:40,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-07 21:26:40,731 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-07 21:26:40,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-07 21:26:40,740 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 07 21:26:40
2023-02-07 21:26:40,743 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-07 21:26:40,743 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-07 21:26:40,746 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-07 21:26:40,746 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-07 21:26:40,766 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-07 21:26:40,772 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-07 21:26:40,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-07 21:26:40,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-07 21:26:40,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-07 21:26:40,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-07 21:26:40,776 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-07 21:26:40,778 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-07 21:26:40,875 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-07 21:26:40,875 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-07 21:26:40,875 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-07 21:26:40,875 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-07 21:26:40,876 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-07 21:26:40,876 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-07 21:26:40,876 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-07 21:26:40,886 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-07 21:26:40,886 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-07 21:26:40,887 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-07 21:26:40,887 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-07 21:26:40,889 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-07 21:26:40,892 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-07 21:26:40,892 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-07 21:26:40,898 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-07 21:26:40,898 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-07 21:26:40,898 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-07 21:26:40,916 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-07 21:26:40,916 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-07 21:26:40,927 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-07 21:26:41,006 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-07 21:26:41,022 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-07 21:26:41,029 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-07 21:26:41,043 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-07 21:26:41,051 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-07 21:26:41,052 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-07 21:26:41,052 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-07 21:26:41,116 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-07 21:26:41,116 INFO org.mortbay.log: jetty-6.1.26
2023-02-07 21:26:41,403 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-07 21:26:41,403 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-07 21:27:41,979 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-07 21:27:42,579 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-07 21:27:42,652 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-07 21:27:43,073 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2023-02-07 21:27:43,073 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 323 bytes.
2023-02-07 21:27:43,083 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=1&endTxId=2&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-07 21:27:43,089 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-07 21:27:43,089 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000001-0000000000000000002_0000000000003714679 size 0 bytes.
2023-02-07 21:27:43,152 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-02-07 21:27:43,202 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-07 21:27:43,202 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000000
2023-02-07 21:27:43,203 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-07 21:27:43,213 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-07 21:27:43,220 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 expecting start txid #1
2023-02-07 21:27:43,221 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002
2023-02-07 21:27:43,248 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000001-0000000000000000002 of size 42 edits # 2 loaded in 0 seconds
2023-02-07 21:27:43,256 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 using no compression
2023-02-07 21:27:43,304 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000002 of size 323 bytes saved in 0 seconds.
2023-02-07 21:27:43,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-07 21:27:43,316 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-07 21:27:43,408 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 2 to namenode at http://localhost:50070 in 0.089 seconds
2023-02-07 21:27:43,408 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 323
2023-02-07 22:10:41,159 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-07 22:10:41,164 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-15 22:32:34,225 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-15 22:32:34,275 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-15 22:32:35,037 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-15 22:32:35,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-15 22:32:35,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-15 22:32:35,419 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-15 22:32:35,433 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 6592@ganesh-virtual-machine
2023-02-15 22:32:35,442 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-15 22:32:35,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-15 22:32:35,446 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-15 22:32:35,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-15 22:32:35,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-15 22:32:35,496 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-15 22:32:35,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 15 22:32:35
2023-02-15 22:32:35,500 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-15 22:32:35,500 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-15 22:32:35,502 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-15 22:32:35,502 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-15 22:32:35,519 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-15 22:32:35,522 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-15 22:32:35,522 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-15 22:32:35,522 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-15 22:32:35,522 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-15 22:32:35,523 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-15 22:32:35,523 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-15 22:32:35,523 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-15 22:32:35,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-15 22:32:35,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-15 22:32:35,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-15 22:32:35,525 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-15 22:32:35,527 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-15 22:32:35,598 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-15 22:32:35,598 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-15 22:32:35,598 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-15 22:32:35,598 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-15 22:32:35,599 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-15 22:32:35,599 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-15 22:32:35,600 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-15 22:32:35,610 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-15 22:32:35,610 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-15 22:32:35,611 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-15 22:32:35,611 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-15 22:32:35,613 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-15 22:32:35,613 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-15 22:32:35,613 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-15 22:32:35,617 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-15 22:32:35,618 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-15 22:32:35,618 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-15 22:32:35,630 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-15 22:32:35,630 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-15 22:32:35,640 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-15 22:32:35,730 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-15 22:32:35,742 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-15 22:32:35,752 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-15 22:32:35,760 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-15 22:32:35,763 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-15 22:32:35,763 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-15 22:32:35,764 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-15 22:32:35,787 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-15 22:32:35,787 INFO org.mortbay.log: jetty-6.1.26
2023-02-15 22:32:36,029 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-15 22:32:36,030 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-15 22:33:36,323 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-15 22:33:36,815 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=3&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-15 22:33:36,937 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-15 22:33:37,390 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2023-02-15 22:33:37,395 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000003 size 323 bytes.
2023-02-15 22:33:37,405 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=4&endTxId=5&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-15 22:33:37,417 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-15 22:33:37,417 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000004-0000000000000000005_0000000000004547142 size 0 bytes.
2023-02-15 22:33:37,482 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-02-15 22:33:37,523 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-15 22:33:37,523 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 3 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000003
2023-02-15 22:33:37,523 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-15 22:33:37,532 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-15 22:33:37,537 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005 expecting start txid #4
2023-02-15 22:33:37,538 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005
2023-02-15 22:33:37,580 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000004-0000000000000000005 of size 42 edits # 2 loaded in 0 seconds
2023-02-15 22:33:37,596 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000005 using no compression
2023-02-15 22:33:37,639 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000005 of size 323 bytes saved in 0 seconds.
2023-02-15 22:33:37,649 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-15 22:33:37,657 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-15 22:33:37,707 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 5 to namenode at http://localhost:50070 in 0.046 seconds
2023-02-15 22:33:37,708 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 323
2023-02-15 22:48:29,261 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-15 22:48:29,623 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-19 21:43:08,015 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-19 21:43:08,070 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-19 21:43:08,867 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-19 21:43:08,979 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-19 21:43:08,979 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-19 21:43:09,284 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-19 21:43:09,302 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3292@ganesh-virtual-machine
2023-02-19 21:43:09,312 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-19 21:43:09,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-19 21:43:09,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-19 21:43:09,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-19 21:43:09,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-19 21:43:09,384 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-19 21:43:09,387 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 19 21:43:09
2023-02-19 21:43:09,389 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-19 21:43:09,390 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-19 21:43:09,392 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-19 21:43:09,392 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-19 21:43:09,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-19 21:43:09,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-19 21:43:09,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-19 21:43:09,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-19 21:43:09,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-19 21:43:09,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-19 21:43:09,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-19 21:43:09,426 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-19 21:43:09,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-19 21:43:09,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-19 21:43:09,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-19 21:43:09,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-19 21:43:09,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-19 21:43:09,518 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-19 21:43:09,518 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-19 21:43:09,518 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-19 21:43:09,519 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-19 21:43:09,519 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-19 21:43:09,519 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-19 21:43:09,520 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-19 21:43:09,531 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-19 21:43:09,531 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-19 21:43:09,532 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-19 21:43:09,532 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-19 21:43:09,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-19 21:43:09,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-19 21:43:09,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-19 21:43:09,540 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-19 21:43:09,540 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-19 21:43:09,540 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-19 21:43:09,555 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-19 21:43:09,555 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-19 21:43:09,567 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-19 21:43:09,675 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-19 21:43:09,693 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-19 21:43:09,702 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-19 21:43:09,713 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-19 21:43:09,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-19 21:43:09,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-19 21:43:09,730 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-19 21:43:09,768 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-19 21:43:09,768 INFO org.mortbay.log: jetty-6.1.26
2023-02-19 21:43:10,267 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-19 21:43:10,267 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-20 00:35:40,012 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-20 00:35:40,654 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=6&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-20 00:35:40,859 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-20 00:35:41,997 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2023-02-20 00:35:41,997 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000006 size 323 bytes.
2023-02-20 00:35:42,012 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=7&endTxId=140&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-20 00:35:42,016 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 7500.00 KB/s
2023-02-20 00:35:42,016 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000007-0000000000000000140_0000000000003639159 size 0 bytes.
2023-02-20 00:35:42,093 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2023-02-20 00:35:42,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-20 00:35:42,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 6 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000006
2023-02-20 00:35:42,218 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-20 00:35:42,231 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-20 00:35:42,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000007-0000000000000000140 expecting start txid #7
2023-02-20 00:35:42,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000007-0000000000000000140
2023-02-20 00:35:42,440 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000007-0000000000000000140 of size 15980 edits # 134 loaded in 0 seconds
2023-02-20 00:35:42,451 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000140 using no compression
2023-02-20 00:35:42,519 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000140 of size 3813 bytes saved in 0 seconds.
2023-02-20 00:35:42,524 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-20 00:35:42,536 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-20 00:35:42,632 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 140 to namenode at http://localhost:50070 in 0.091 seconds
2023-02-20 00:35:42,633 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 3813
2023-02-20 00:46:18,155 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-20 00:46:18,206 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-21 13:06:49,271 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-21 13:06:49,331 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-21 13:06:50,156 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-21 13:06:50,263 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-21 13:06:50,263 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-21 13:06:50,533 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-21 13:06:50,547 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3998@ganesh-virtual-machine
2023-02-21 13:06:50,558 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-21 13:06:50,560 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-21 13:06:50,562 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-21 13:06:50,617 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-21 13:06:50,617 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-21 13:06:50,622 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-21 13:06:50,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 21 13:06:50
2023-02-21 13:06:50,626 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-21 13:06:50,627 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:06:50,629 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-21 13:06:50,629 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-21 13:06:50,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-21 13:06:50,654 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-21 13:06:50,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-21 13:06:50,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-21 13:06:50,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-21 13:06:50,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-21 13:06:50,659 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-21 13:06:50,756 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-21 13:06:50,756 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:06:50,756 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-21 13:06:50,756 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-21 13:06:50,757 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-21 13:06:50,758 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-21 13:06:50,758 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-21 13:06:50,768 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-21 13:06:50,768 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:06:50,769 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-21 13:06:50,769 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-21 13:06:50,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-21 13:06:50,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-21 13:06:50,775 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-21 13:06:50,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-21 13:06:50,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-21 13:06:50,782 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-21 13:06:50,806 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-21 13:06:50,806 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-21 13:06:50,821 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-21 13:06:50,917 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-21 13:06:50,934 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-21 13:06:50,948 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-21 13:06:50,958 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-21 13:06:50,964 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-21 13:06:50,964 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-21 13:06:50,965 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-21 13:06:51,001 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-21 13:06:51,001 INFO org.mortbay.log: jetty-6.1.26
2023-02-21 13:06:51,438 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-21 13:06:51,438 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-21 13:17:26,758 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-21 13:17:26,852 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-21 13:21:32,879 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-21 13:21:32,893 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-21 13:21:33,720 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-21 13:21:33,850 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-21 13:21:33,850 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-21 13:21:34,146 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-21 13:21:34,163 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3012@ganesh-virtual-machine
2023-02-21 13:21:34,174 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-21 13:21:34,176 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-21 13:21:34,179 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-21 13:21:34,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-21 13:21:34,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-21 13:21:34,255 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-21 13:21:34,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 21 13:21:34
2023-02-21 13:21:34,260 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-21 13:21:34,261 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:21:34,263 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-21 13:21:34,263 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-21 13:21:34,286 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-21 13:21:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-21 13:21:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-21 13:21:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-21 13:21:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-21 13:21:34,290 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-21 13:21:34,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-21 13:21:34,291 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-21 13:21:34,294 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-21 13:21:34,294 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-21 13:21:34,294 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-21 13:21:34,294 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-21 13:21:34,297 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-21 13:21:34,389 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-21 13:21:34,389 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:21:34,390 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-21 13:21:34,390 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-21 13:21:34,391 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-21 13:21:34,391 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-21 13:21:34,391 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-21 13:21:34,402 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-21 13:21:34,403 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-21 13:21:34,403 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-21 13:21:34,403 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-21 13:21:34,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-21 13:21:34,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-21 13:21:34,405 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-21 13:21:34,414 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-21 13:21:34,414 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-21 13:21:34,414 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-21 13:21:34,430 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-21 13:21:34,430 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-21 13:21:34,443 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-21 13:21:34,570 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-21 13:21:34,598 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-21 13:21:34,612 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-21 13:21:34,629 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-21 13:21:34,636 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-21 13:21:34,636 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-21 13:21:34,637 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-21 13:21:34,687 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-21 13:21:34,687 INFO org.mortbay.log: jetty-6.1.26
2023-02-21 13:21:35,085 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-21 13:21:35,086 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-21 15:42:06,498 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-21 15:42:06,586 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-23 12:30:53,509 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-23 12:30:53,522 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-23 12:30:54,270 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-23 12:30:54,369 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-23 12:30:54,370 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-23 12:30:54,641 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-23 12:30:54,656 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 7514@ganesh-virtual-machine
2023-02-23 12:30:54,665 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-23 12:30:54,666 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-23 12:30:54,669 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-23 12:30:54,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-23 12:30:54,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-23 12:30:54,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-23 12:30:54,722 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 23 12:30:54
2023-02-23 12:30:54,725 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-23 12:30:54,725 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 12:30:54,727 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-23 12:30:54,727 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-23 12:30:54,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-23 12:30:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-23 12:30:54,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-23 12:30:54,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-23 12:30:54,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-23 12:30:54,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-23 12:30:54,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-23 12:30:54,831 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-23 12:30:54,831 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 12:30:54,831 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-23 12:30:54,831 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-23 12:30:54,833 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-23 12:30:54,833 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-23 12:30:54,833 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-23 12:30:54,843 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-23 12:30:54,843 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 12:30:54,843 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-23 12:30:54,843 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-23 12:30:54,845 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-23 12:30:54,846 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-23 12:30:54,846 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-23 12:30:54,851 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-23 12:30:54,851 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-23 12:30:54,851 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-23 12:30:54,868 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-23 12:30:54,868 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-23 12:30:54,878 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-23 12:30:54,966 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-23 12:30:54,978 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-23 12:30:54,988 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-23 12:30:54,996 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-23 12:30:54,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-23 12:30:54,998 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-23 12:30:54,999 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-23 12:30:55,019 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-23 12:30:55,019 INFO org.mortbay.log: jetty-6.1.26
2023-02-23 12:30:55,710 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-23 12:30:55,710 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-23 12:42:14,948 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-23 12:42:14,973 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-23 22:21:40,743 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-23 22:21:40,755 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-23 22:21:41,431 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-23 22:21:41,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-23 22:21:41,618 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-23 22:21:41,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-23 22:21:41,902 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 5027@ganesh-virtual-machine
2023-02-23 22:21:41,912 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-23 22:21:41,913 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-23 22:21:41,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-23 22:21:41,963 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-23 22:21:41,963 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-23 22:21:41,967 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-23 22:21:41,969 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 23 22:21:41
2023-02-23 22:21:41,971 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-23 22:21:41,972 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 22:21:41,974 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-23 22:21:41,974 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-23 22:21:41,990 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-23 22:21:41,994 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-23 22:21:41,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-23 22:21:41,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-23 22:21:41,997 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-23 22:21:41,998 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-23 22:21:41,999 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-23 22:21:42,099 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-23 22:21:42,099 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 22:21:42,099 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-23 22:21:42,099 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-23 22:21:42,100 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-23 22:21:42,100 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-23 22:21:42,100 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-23 22:21:42,110 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-23 22:21:42,110 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-23 22:21:42,110 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-23 22:21:42,110 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-23 22:21:42,112 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-23 22:21:42,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-23 22:21:42,115 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-23 22:21:42,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-23 22:21:42,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-23 22:21:42,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-23 22:21:42,134 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-23 22:21:42,134 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-23 22:21:42,143 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-23 22:21:42,215 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-23 22:21:42,225 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-23 22:21:42,231 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-23 22:21:42,239 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-23 22:21:42,242 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-23 22:21:42,242 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-23 22:21:42,242 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-23 22:21:42,262 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-23 22:21:42,262 INFO org.mortbay.log: jetty-6.1.26
2023-02-23 22:21:42,542 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-23 22:21:42,543 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-23 22:58:09,823 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-23 22:58:10,415 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=144&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-23 22:58:10,467 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-23 22:58:10,935 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 300.00 KB/s
2023-02-23 22:58:10,935 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000144 size 3813 bytes.
2023-02-23 22:58:10,955 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=145&endTxId=146&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-23 22:58:10,968 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-23 22:58:10,968 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000145-0000000000000000146_0000000000003659486 size 0 bytes.
2023-02-23 22:58:11,049 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 46 INodes.
2023-02-23 22:58:11,135 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-23 22:58:11,136 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 144 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000144
2023-02-23 22:58:11,136 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-23 22:58:11,151 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-23 22:58:11,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000145-0000000000000000146 expecting start txid #145
2023-02-23 22:58:11,163 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000145-0000000000000000146
2023-02-23 22:58:11,201 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000145-0000000000000000146 of size 42 edits # 2 loaded in 0 seconds
2023-02-23 22:58:11,212 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000146 using no compression
2023-02-23 22:58:11,273 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000146 of size 3813 bytes saved in 0 seconds.
2023-02-23 22:58:11,278 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-23 22:58:11,286 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-23 22:58:11,329 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 146 to namenode at http://localhost:50070 in 0.04 seconds
2023-02-23 22:58:11,330 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 3813
2023-02-24 00:27:14,546 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-24 00:27:14,593 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-24 01:57:42,553 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-24 01:57:42,604 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-24 01:57:43,342 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-24 01:57:43,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-24 01:57:43,436 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-24 01:57:43,699 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-24 01:57:43,713 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3325@ganesh-virtual-machine
2023-02-24 01:57:43,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-24 01:57:43,725 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-24 01:57:43,728 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-24 01:57:43,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-24 01:57:43,776 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-24 01:57:43,778 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-24 01:57:43,781 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 24 01:57:43
2023-02-24 01:57:43,783 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-24 01:57:43,783 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 01:57:43,785 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-24 01:57:43,785 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-24 01:57:43,802 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-24 01:57:43,805 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-24 01:57:43,806 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-24 01:57:43,808 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-24 01:57:43,808 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-24 01:57:43,808 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-24 01:57:43,808 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-24 01:57:43,810 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-24 01:57:43,882 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-24 01:57:43,882 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 01:57:43,882 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-24 01:57:43,882 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-24 01:57:43,883 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-24 01:57:43,883 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-24 01:57:43,884 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-24 01:57:43,894 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-24 01:57:43,894 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 01:57:43,894 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-24 01:57:43,894 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-24 01:57:43,896 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-24 01:57:43,896 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-24 01:57:43,897 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-24 01:57:43,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-24 01:57:43,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-24 01:57:43,902 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-24 01:57:43,914 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-24 01:57:43,914 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-24 01:57:43,924 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-24 01:57:43,999 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-24 01:57:44,009 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-24 01:57:44,024 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-24 01:57:44,031 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-24 01:57:44,034 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-24 01:57:44,034 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-24 01:57:44,034 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-24 01:57:44,056 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-24 01:57:44,056 INFO org.mortbay.log: jetty-6.1.26
2023-02-24 01:57:44,639 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-24 01:57:44,639 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-24 02:03:26,694 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-24 02:03:26,700 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-24 02:05:06,129 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-24 02:05:06,140 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-24 02:05:06,881 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-24 02:05:06,991 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-24 02:05:06,991 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-24 02:05:07,236 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-24 02:05:07,251 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 5475@ganesh-virtual-machine
2023-02-24 02:05:07,260 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-24 02:05:07,262 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-24 02:05:07,264 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-24 02:05:07,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-24 02:05:07,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-24 02:05:07,318 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-24 02:05:07,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 24 02:05:07
2023-02-24 02:05:07,322 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-24 02:05:07,322 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 02:05:07,324 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-24 02:05:07,324 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-24 02:05:07,343 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-24 02:05:07,346 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-24 02:05:07,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-24 02:05:07,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-24 02:05:07,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-24 02:05:07,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-24 02:05:07,351 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-24 02:05:07,419 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-24 02:05:07,419 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 02:05:07,419 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-24 02:05:07,419 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-24 02:05:07,420 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-24 02:05:07,420 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-24 02:05:07,420 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-24 02:05:07,430 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-24 02:05:07,430 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 02:05:07,430 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-24 02:05:07,430 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-24 02:05:07,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-24 02:05:07,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-24 02:05:07,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-24 02:05:07,437 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-24 02:05:07,437 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-24 02:05:07,437 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-24 02:05:07,450 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-24 02:05:07,450 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-24 02:05:07,462 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-24 02:05:07,536 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-24 02:05:07,546 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-24 02:05:07,559 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-24 02:05:07,568 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-24 02:05:07,570 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-24 02:05:07,571 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-24 02:05:07,571 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-24 02:05:07,591 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-24 02:05:07,591 INFO org.mortbay.log: jetty-6.1.26
2023-02-24 02:05:07,798 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-24 02:05:07,798 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-24 02:27:58,280 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-24 02:27:58,297 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-24 13:20:34,203 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-24 13:20:34,234 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-24 13:20:35,144 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-24 13:20:35,262 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-24 13:20:35,263 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-24 13:20:35,624 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-24 13:20:35,642 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3614@ganesh-virtual-machine
2023-02-24 13:20:35,655 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-24 13:20:35,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-24 13:20:35,660 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-24 13:20:35,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-24 13:20:35,718 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-24 13:20:35,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-24 13:20:35,723 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 24 13:20:35
2023-02-24 13:20:35,725 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-24 13:20:35,725 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 13:20:35,728 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-24 13:20:35,728 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-24 13:20:35,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-24 13:20:35,759 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-24 13:20:35,759 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-24 13:20:35,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-24 13:20:35,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-24 13:20:35,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-24 13:20:35,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-24 13:20:35,760 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-24 13:20:35,762 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-24 13:20:35,762 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-24 13:20:35,762 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-24 13:20:35,762 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-24 13:20:35,765 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-24 13:20:35,853 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-24 13:20:35,853 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 13:20:35,853 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-24 13:20:35,853 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-24 13:20:35,854 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-24 13:20:35,854 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-24 13:20:35,854 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-24 13:20:35,867 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-24 13:20:35,868 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 13:20:35,868 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-24 13:20:35,868 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-24 13:20:35,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-24 13:20:35,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-24 13:20:35,870 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-24 13:20:35,996 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-24 13:20:35,996 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-24 13:20:35,996 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-24 13:20:36,011 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-24 13:20:36,012 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-24 13:20:36,024 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-24 13:20:36,111 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-24 13:20:36,124 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-24 13:20:36,139 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-24 13:20:36,148 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-24 13:20:36,151 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-24 13:20:36,151 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-24 13:20:36,151 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-24 13:20:36,180 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-24 13:20:36,180 INFO org.mortbay.log: jetty-6.1.26
2023-02-24 13:20:36,357 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-24 13:20:36,357 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-24 13:21:37,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:38,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:39,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:40,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:41,544 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:42,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:43,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:44,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:45,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:46,564 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:21:46,566 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:21:46,627 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:22:47,641 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:48,642 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:49,644 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:50,646 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:51,648 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:52,650 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:53,652 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:54,653 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:55,655 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:56,656 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:22:56,657 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:22:56,662 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:23:57,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:23:58,670 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:23:59,671 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:00,672 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:01,678 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:02,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:03,680 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:04,682 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:05,683 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:06,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:24:06,685 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:24:06,686 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:25:07,692 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:08,698 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:09,700 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:10,701 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:11,702 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:12,707 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:13,710 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:14,712 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:15,714 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:16,715 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:25:16,716 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:25:16,719 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:26:17,724 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:18,725 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:19,727 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:20,729 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:21,730 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:22,731 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:23,736 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:24,737 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:25,738 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:26,739 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:26:26,740 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:26:26,743 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:27:27,747 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:28,748 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:29,753 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:30,755 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:31,756 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:32,758 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:33,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:34,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:35,794 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:36,795 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:27:36,796 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:27:36,797 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:28:37,802 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:38,809 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:39,810 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:40,812 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:41,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:42,817 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:43,818 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:44,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:45,820 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:46,821 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:28:46,822 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:28:46,823 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:29:47,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:48,831 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:49,832 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:50,833 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:51,837 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:52,841 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:53,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:54,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:55,849 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:56,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:29:56,852 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:29:56,853 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:30:57,857 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:30:58,858 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:30:59,860 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:00,861 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:01,862 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:02,863 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:03,864 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:04,866 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:05,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:06,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:31:06,868 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:31:06,870 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:32:07,876 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:08,878 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:09,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:10,881 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:11,882 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:12,884 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:13,886 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:14,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:15,889 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:16,890 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:32:16,891 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:32:16,893 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:33:17,898 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:18,899 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:19,900 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:20,902 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:21,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:22,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:23,906 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:24,908 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:25,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:26,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:33:26,912 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:33:26,914 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:34:27,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:28,925 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:29,927 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:30,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:31,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:32,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:33,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:34,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:35,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:36,938 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:34:36,939 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:34:36,941 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:35:37,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:38,947 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:39,948 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:40,949 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:41,951 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:42,953 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:43,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:44,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:45,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:46,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:35:46,959 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:35:46,960 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:36:47,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:48,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:49,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:50,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:51,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:52,971 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:53,972 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:54,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:55,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:56,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:36:56,978 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:36:56,979 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:37:57,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:37:58,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:37:59,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:00,991 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:01,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:02,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:04,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:05,825 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:06,829 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:07,830 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:38:07,831 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:38:07,832 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:39:08,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:09,842 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:10,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:11,844 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:12,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:13,848 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:14,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:15,852 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:17,335 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:18,339 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:39:18,340 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:39:18,341 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 22 more
2023-02-24 13:40:19,345 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:20,346 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:21,348 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:22,349 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:23,350 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:24,352 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:25,353 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:26,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:27,355 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:28,356 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:40:28,357 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:40:28,358 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor6.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2023-02-24 13:41:29,361 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:30,362 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:31,363 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:32,365 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:33,366 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:34,368 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:36,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:37,047 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:38,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:39,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:41:39,050 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:41:39,052 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor6.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2023-02-24 13:42:40,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:41,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:42,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:43,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:44,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:45,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:46,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:47,066 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:48,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:49,068 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2023-02-24 13:42:49,069 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
2023-02-24 13:42:49,070 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
java.net.ConnectException: Call From ganesh-virtual-machine/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor6.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1485)
	at org.apache.hadoop.ipc.Client.call(Client.java:1427)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getTransactionId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:127)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy11.getTransactionID(Unknown Source)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:650)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:658)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:358)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$1.run(SecondaryNameNode.java:325)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:455)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:321)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:681)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:777)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:409)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1542)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	... 21 more
2023-02-24 13:43:32,291 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-24 13:43:32,318 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-24 18:46:28,349 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-24 18:46:28,361 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-24 18:46:29,179 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-24 18:46:29,300 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-24 18:46:29,300 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-24 18:46:29,531 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-24 18:46:29,545 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 3656@ganesh-virtual-machine
2023-02-24 18:46:29,553 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-24 18:46:29,554 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-24 18:46:29,557 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-24 18:46:29,605 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-24 18:46:29,605 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-24 18:46:29,607 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-24 18:46:29,609 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 24 18:46:29
2023-02-24 18:46:29,611 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-24 18:46:29,611 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 18:46:29,613 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-24 18:46:29,613 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-24 18:46:29,630 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-24 18:46:29,633 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-24 18:46:29,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-24 18:46:29,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-24 18:46:29,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-24 18:46:29,635 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-24 18:46:29,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-24 18:46:29,719 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-24 18:46:29,719 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 18:46:29,720 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-24 18:46:29,720 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-24 18:46:29,720 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-24 18:46:29,721 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-24 18:46:29,721 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-24 18:46:29,730 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-24 18:46:29,730 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-24 18:46:29,731 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-24 18:46:29,731 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-24 18:46:29,733 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-24 18:46:29,733 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-24 18:46:29,733 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-24 18:46:29,737 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-24 18:46:29,737 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-24 18:46:29,737 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-24 18:46:29,749 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-24 18:46:29,749 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-24 18:46:29,760 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-24 18:46:29,842 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-24 18:46:29,897 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-24 18:46:29,908 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-24 18:46:29,919 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-24 18:46:29,933 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-24 18:46:29,933 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-24 18:46:29,933 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-24 18:46:29,959 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-24 18:46:29,959 INFO org.mortbay.log: jetty-6.1.26
2023-02-24 18:46:30,443 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-24 18:46:30,443 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-24 19:21:31,526 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-24 19:21:32,080 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=239&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-24 19:21:32,177 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-24 19:21:33,016 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.02s at 312.50 KB/s
2023-02-24 19:21:33,016 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000239 size 5259 bytes.
2023-02-24 19:21:33,045 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=240&endTxId=262&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-24 19:21:33,057 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 1000.00 KB/s
2023-02-24 19:21:33,057 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000240-0000000000000000262_0000000000003611224 size 0 bytes.
2023-02-24 19:21:33,184 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 60 INodes.
2023-02-24 19:21:33,386 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-24 19:21:33,387 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 239 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000239
2023-02-24 19:21:33,387 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-24 19:21:33,418 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-24 19:21:33,432 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000240-0000000000000000262 expecting start txid #240
2023-02-24 19:21:33,433 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000240-0000000000000000262
2023-02-24 19:21:33,605 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000240-0000000000000000262 of size 2340 edits # 23 loaded in 0 seconds
2023-02-24 19:21:33,619 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000262 using no compression
2023-02-24 19:21:33,730 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000262 of size 5527 bytes saved in 0 seconds.
2023-02-24 19:21:33,737 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-24 19:21:33,751 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-24 19:21:33,838 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 262 to namenode at http://localhost:50070 in 0.079 seconds
2023-02-24 19:21:33,839 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5527
2023-02-24 20:36:59,284 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-24 20:36:59,285 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=263&endTxId=273&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-24 20:36:59,295 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 250.00 KB/s
2023-02-24 20:36:59,296 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000263-0000000000000000273_0000000000007212613 size 0 bytes.
2023-02-24 20:36:59,296 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-24 20:36:59,296 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000263-0000000000000000273 expecting start txid #263
2023-02-24 20:36:59,297 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000263-0000000000000000273
2023-02-24 20:36:59,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000263-0000000000000000273 of size 1235 edits # 11 loaded in 0 seconds
2023-02-24 20:36:59,304 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000273 using no compression
2023-02-24 20:36:59,319 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000273 of size 5051 bytes saved in 0 seconds.
2023-02-24 20:36:59,334 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 262
2023-02-24 20:36:59,334 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000239, cpktTxId=0000000000000000239)
2023-02-24 20:36:59,368 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 273 to namenode at http://localhost:50070 in 0.022 seconds
2023-02-24 20:36:59,368 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5051
2023-02-24 22:49:14,336 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-24 22:49:14,337 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=274&endTxId=275&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-24 22:49:14,358 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-24 22:49:14,359 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000274-0000000000000000275_0000000000010855556 size 0 bytes.
2023-02-24 22:49:14,359 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-24 22:49:14,359 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000274-0000000000000000275 expecting start txid #274
2023-02-24 22:49:14,360 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000274-0000000000000000275
2023-02-24 22:49:14,361 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000274-0000000000000000275 of size 42 edits # 2 loaded in 0 seconds
2023-02-24 22:49:14,363 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000275 using no compression
2023-02-24 22:49:14,373 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000275 of size 5051 bytes saved in 0 seconds.
2023-02-24 22:49:14,381 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 273
2023-02-24 22:49:14,381 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000262, cpktTxId=0000000000000000262)
2023-02-24 22:49:14,406 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 275 to namenode at http://localhost:50070 in 0.022 seconds
2023-02-24 22:49:14,406 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5051
2023-02-24 23:58:05,979 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-24 23:58:05,981 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=276&endTxId=277&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-24 23:58:05,995 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-24 23:58:05,995 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000276-0000000000000000277_0000000000014457317 size 0 bytes.
2023-02-24 23:58:05,996 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-24 23:58:05,996 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000276-0000000000000000277 expecting start txid #276
2023-02-24 23:58:05,996 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000276-0000000000000000277
2023-02-24 23:58:05,997 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000276-0000000000000000277 of size 42 edits # 2 loaded in 0 seconds
2023-02-24 23:58:06,000 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000277 using no compression
2023-02-24 23:58:06,032 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000277 of size 5051 bytes saved in 0 seconds.
2023-02-24 23:58:06,037 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 275
2023-02-24 23:58:06,038 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000273, cpktTxId=0000000000000000273)
2023-02-24 23:58:06,066 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 277 to namenode at http://localhost:50070 in 0.02 seconds
2023-02-24 23:58:06,066 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5051
2023-02-25 01:37:12,454 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: RECEIVED SIGNAL 15: SIGTERM
2023-02-25 01:37:12,552 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at ganesh-virtual-machine/127.0.1.1
************************************************************/
2023-02-25 15:11:33,752 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting SecondaryNameNode
STARTUP_MSG:   user = ganesh
STARTUP_MSG:   host = ganesh-virtual-machine/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.1
STARTUP_MSG:   classpath = /home/ganesh/hadoop-2.8.1/etc/hadoop:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/nimbus-jose-jwt-3.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-digester-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-net-3.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jcip-annotations-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsp-api-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/gson-2.2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/hadoop-auth-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/json-smart-1.1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/lib/jsch-0.1.51.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/common/hadoop-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jettison-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/objenesis-2.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/fst-2.24.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/activation-1.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-registry-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-api-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-client-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/yarn/hadoop-yarn-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.1-tests.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.1.jar:/home/ganesh/hadoop-2.8.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.1.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 20fe5304904fc2f5a18053c389e43cd26f7a70fe; compiled by 'vinodkv' on 2017-06-02T06:14Z
STARTUP_MSG:   java = 1.8.0_352
************************************************************/
2023-02-25 15:11:33,789 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2023-02-25 15:11:34,719 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2023-02-25 15:11:34,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2023-02-25 15:11:34,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: SecondaryNameNode metrics system started
2023-02-25 15:11:35,121 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false
2023-02-25 15:11:35,138 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-ganesh/dfs/namesecondary/in_use.lock acquired by nodename 4203@ganesh-virtual-machine
2023-02-25 15:11:35,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null
2023-02-25 15:11:35,151 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2023-02-25 15:11:35,154 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2023-02-25 15:11:35,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2023-02-25 15:11:35,212 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2023-02-25 15:11:35,214 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2023-02-25 15:11:35,217 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2023 Feb 25 15:11:35
2023-02-25 15:11:35,219 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2023-02-25 15:11:35,219 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-25 15:11:35,221 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2023-02-25 15:11:35,221 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2023-02-25 15:11:35,245 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2023-02-25 15:11:35,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2023-02-25 15:11:35,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = ganesh (auth:SIMPLE)
2023-02-25 15:11:35,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2023-02-25 15:11:35,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2023-02-25 15:11:35,251 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2023-02-25 15:11:35,253 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2023-02-25 15:11:35,339 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2023-02-25 15:11:35,339 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-25 15:11:35,339 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2023-02-25 15:11:35,339 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2023-02-25 15:11:35,340 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2023-02-25 15:11:35,340 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2023-02-25 15:11:35,341 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than 10 times
2023-02-25 15:11:35,353 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2023-02-25 15:11:35,353 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2023-02-25 15:11:35,353 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2023-02-25 15:11:35,353 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2023-02-25 15:11:35,355 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2023-02-25 15:11:35,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2023-02-25 15:11:35,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2023-02-25 15:11:35,361 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2023-02-25 15:11:35,361 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2023-02-25 15:11:35,361 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2023-02-25 15:11:35,375 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period   :3600 secs (60 min)
2023-02-25 15:11:35,376 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger    :1000000 txns
2023-02-25 15:11:35,386 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for secondary at: http://0.0.0.0:50090
2023-02-25 15:11:35,473 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2023-02-25 15:11:35,486 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2023-02-25 15:11:35,498 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.secondary is not defined
2023-02-25 15:11:35,507 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2023-02-25 15:11:35,510 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context secondary
2023-02-25 15:11:35,511 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2023-02-25 15:11:35,511 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2023-02-25 15:11:35,541 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50090
2023-02-25 15:11:35,541 INFO org.mortbay.log: jetty-6.1.26
2023-02-25 15:11:35,944 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50090
2023-02-25 15:11:35,944 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done
2023-02-25 16:03:30,118 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.
2023-02-25 16:03:30,692 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getimage=1&txid=278&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408&bootstrapstandby=false
2023-02-25 16:03:30,772 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to 60000 milliseconds
2023-02-25 16:03:31,545 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 500.00 KB/s
2023-02-25 16:03:31,545 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000278 size 5051 bytes.
2023-02-25 16:03:31,555 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=279&endTxId=302&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-25 16:03:31,559 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 2000.00 KB/s
2023-02-25 16:03:31,559 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000279-0000000000000000302_0000000000003603268 size 0 bytes.
2023-02-25 16:03:31,642 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 64 INodes.
2023-02-25 16:03:31,868 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2023-02-25 16:03:31,868 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 278 from /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000278
2023-02-25 16:03:31,868 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2023-02-25 16:03:31,879 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-25 16:03:31,888 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000279-0000000000000000302 expecting start txid #279
2023-02-25 16:03:31,888 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000279-0000000000000000302
2023-02-25 16:03:32,047 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000279-0000000000000000302 of size 2475 edits # 24 loaded in 0 seconds
2023-02-25 16:03:32,060 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000302 using no compression
2023-02-25 16:03:32,117 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000302 of size 5378 bytes saved in 0 seconds.
2023-02-25 16:03:32,124 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-25 16:03:32,132 INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in /tmp/hadoop-ganesh/dfs/namesecondary
2023-02-25 16:03:32,186 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 302 to namenode at http://localhost:50070 in 0.05 seconds
2023-02-25 16:03:32,187 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5378
2023-02-25 17:03:33,034 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-25 17:03:33,036 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=303&endTxId=305&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-25 17:03:33,041 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-25 17:03:33,041 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000303-0000000000000000305_0000000000007204749 size 0 bytes.
2023-02-25 17:03:33,041 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-25 17:03:33,042 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000303-0000000000000000305 expecting start txid #303
2023-02-25 17:03:33,042 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000303-0000000000000000305
2023-02-25 17:03:33,047 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000303-0000000000000000305 of size 104 edits # 3 loaded in 0 seconds
2023-02-25 17:03:33,049 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000305 using no compression
2023-02-25 17:03:33,063 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000305 of size 5378 bytes saved in 0 seconds.
2023-02-25 17:03:33,069 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 302
2023-02-25 17:03:33,070 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000278, cpktTxId=0000000000000000278)
2023-02-25 17:03:33,094 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 305 to namenode at http://localhost:50070 in 0.016 seconds
2023-02-25 17:03:33,094 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5378
2023-02-25 18:56:42,315 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-25 18:56:42,322 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=306&endTxId=315&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-25 18:56:42,340 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-25 18:56:42,340 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000306-0000000000000000315_0000000000010805803 size 0 bytes.
2023-02-25 18:56:42,341 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-25 18:56:42,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000306-0000000000000000315 expecting start txid #306
2023-02-25 18:56:42,342 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000306-0000000000000000315
2023-02-25 18:56:42,359 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000306-0000000000000000315 of size 919 edits # 10 loaded in 0 seconds
2023-02-25 18:56:42,361 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000315 using no compression
2023-02-25 18:56:42,377 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000315 of size 5499 bytes saved in 0 seconds.
2023-02-25 18:56:42,383 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 305
2023-02-25 18:56:42,383 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000302, cpktTxId=0000000000000000302)
2023-02-25 18:56:42,431 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 315 to namenode at http://localhost:50070 in 0.043 seconds
2023-02-25 18:56:42,431 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5499
2023-02-25 20:42:54,023 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-25 20:42:54,023 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=316&endTxId=320&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-25 20:42:54,033 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-25 20:42:54,033 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000316-0000000000000000320_0000000000014408421 size 0 bytes.
2023-02-25 20:42:54,034 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-25 20:42:54,034 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000316-0000000000000000320 expecting start txid #316
2023-02-25 20:42:54,035 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000316-0000000000000000320
2023-02-25 20:42:54,042 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000316-0000000000000000320 of size 206 edits # 5 loaded in 0 seconds
2023-02-25 20:42:54,045 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000320 using no compression
2023-02-25 20:42:54,064 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000320 of size 5499 bytes saved in 0 seconds.
2023-02-25 20:42:54,072 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 315
2023-02-25 20:42:54,072 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000305, cpktTxId=0000000000000000305)
2023-02-25 20:42:54,112 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 320 to namenode at http://localhost:50070 in 0.036 seconds
2023-02-25 20:42:54,112 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5499
2023-02-25 21:42:57,493 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-25 21:42:57,497 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=321&endTxId=323&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-25 21:42:57,510 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-25 21:42:57,511 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000321-0000000000000000323_0000000000018011893 size 0 bytes.
2023-02-25 21:42:57,512 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-25 21:42:57,512 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000321-0000000000000000323 expecting start txid #321
2023-02-25 21:42:57,512 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000321-0000000000000000323
2023-02-25 21:42:57,514 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000321-0000000000000000323 of size 104 edits # 3 loaded in 0 seconds
2023-02-25 21:42:57,524 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000323 using no compression
2023-02-25 21:42:57,542 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000323 of size 5499 bytes saved in 0 seconds.
2023-02-25 21:42:57,549 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 320
2023-02-25 21:42:57,550 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000315, cpktTxId=0000000000000000315)
2023-02-25 21:42:57,594 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 323 to namenode at http://localhost:50070 in 0.039 seconds
2023-02-25 21:42:57,594 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5499
2023-02-26 14:04:42,222 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-26 14:04:42,227 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=324&endTxId=325&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-26 14:04:42,246 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-26 14:04:42,246 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000324-0000000000000000325_0000000000021626463 size 0 bytes.
2023-02-26 14:04:42,247 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-26 14:04:42,247 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000324-0000000000000000325 expecting start txid #324
2023-02-26 14:04:42,248 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000324-0000000000000000325
2023-02-26 14:04:42,251 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000324-0000000000000000325 of size 42 edits # 2 loaded in 0 seconds
2023-02-26 14:04:42,254 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000325 using no compression
2023-02-26 14:04:42,287 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000325 of size 5499 bytes saved in 0 seconds.
2023-02-26 14:04:42,302 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 323
2023-02-26 14:04:42,302 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000320, cpktTxId=0000000000000000320)
2023-02-26 14:04:42,358 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 325 to namenode at http://localhost:50070 in 0.036 seconds
2023-02-26 14:04:42,359 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5499
2023-02-26 15:12:13,452 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-26 15:12:13,452 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=326&endTxId=349&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-26 15:12:13,460 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 666.67 KB/s
2023-02-26 15:12:13,460 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000326-0000000000000000349_0000000000025227495 size 0 bytes.
2023-02-26 15:12:13,460 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-26 15:12:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000326-0000000000000000349 expecting start txid #326
2023-02-26 15:12:13,460 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000326-0000000000000000349
2023-02-26 15:12:13,484 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000326-0000000000000000349 of size 2803 edits # 24 loaded in 0 seconds
2023-02-26 15:12:13,485 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000349 using no compression
2023-02-26 15:12:13,490 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000349 of size 5848 bytes saved in 0 seconds.
2023-02-26 15:12:13,498 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 325
2023-02-26 15:12:13,498 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000323, cpktTxId=0000000000000000323)
2023-02-26 15:12:13,514 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 349 to namenode at http://localhost:50070 in 0.013 seconds
2023-02-26 15:12:13,514 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5848
2023-02-26 16:14:11,545 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-26 16:14:11,552 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=350&endTxId=352&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-26 16:14:11,568 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-26 16:14:11,568 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000350-0000000000000000352_0000000000028834414 size 0 bytes.
2023-02-26 16:14:11,569 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-26 16:14:11,569 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000350-0000000000000000352 expecting start txid #350
2023-02-26 16:14:11,570 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000350-0000000000000000352
2023-02-26 16:14:11,573 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000350-0000000000000000352 of size 108 edits # 3 loaded in 0 seconds
2023-02-26 16:14:11,575 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000352 using no compression
2023-02-26 16:14:11,597 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000352 of size 5848 bytes saved in 0 seconds.
2023-02-26 16:14:11,602 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 349
2023-02-26 16:14:11,602 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000325, cpktTxId=0000000000000000325)
2023-02-26 16:14:11,638 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 352 to namenode at http://localhost:50070 in 0.026 seconds
2023-02-26 16:14:11,638 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5848
2023-02-26 17:14:13,379 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-26 17:14:13,380 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=353&endTxId=354&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-26 17:14:13,390 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-26 17:14:13,391 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000353-0000000000000000354_0000000000032436242 size 0 bytes.
2023-02-26 17:14:13,391 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-26 17:14:13,392 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000353-0000000000000000354 expecting start txid #353
2023-02-26 17:14:13,392 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000353-0000000000000000354
2023-02-26 17:14:13,392 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000353-0000000000000000354 of size 42 edits # 2 loaded in 0 seconds
2023-02-26 17:14:13,395 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000354 using no compression
2023-02-26 17:14:13,424 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000354 of size 5848 bytes saved in 0 seconds.
2023-02-26 17:14:13,432 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 352
2023-02-26 17:14:13,432 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000349, cpktTxId=0000000000000000349)
2023-02-26 17:14:13,460 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 354 to namenode at http://localhost:50070 in 0.025 seconds
2023-02-26 17:14:13,460 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5848
2023-02-26 18:14:40,523 INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has not changed. Will not download image.
2023-02-26 18:14:40,536 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to http://localhost:50070/imagetransfer?getedit=1&startTxId=355&endTxId=356&storageInfo=-63:648504085:1675826040519:CID-4db4ccb7-1da9-415d-86dd-c8681c9bf408
2023-02-26 18:14:40,577 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2023-02-26 18:14:40,577 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file edits_tmp_0000000000000000355-0000000000000000356_0000000000036063397 size 0 bytes.
2023-02-26 18:14:40,579 INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from 1 stream(s).
2023-02-26 18:14:40,582 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000355-0000000000000000356 expecting start txid #355
2023-02-26 18:14:40,583 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000355-0000000000000000356
2023-02-26 18:14:40,589 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-ganesh/dfs/namesecondary/current/edits_0000000000000000355-0000000000000000356 of size 42 edits # 2 loaded in 0 seconds
2023-02-26 18:14:40,599 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000356 using no compression
2023-02-26 18:14:40,662 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage.ckpt_0000000000000000356 of size 5848 bytes saved in 0 seconds.
2023-02-26 18:14:40,669 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 354
2023-02-26 18:14:40,669 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-ganesh/dfs/namesecondary/current/fsimage_0000000000000000352, cpktTxId=0000000000000000352)
2023-02-26 18:14:40,715 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid 356 to namenode at http://localhost:50070 in 0.039 seconds
2023-02-26 18:14:40,715 WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: 5848
